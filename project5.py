# -*- coding: utf-8 -*-
"""Project5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C2IGo00YUoXPSw2799LrLV6JH_duAVpJ

#Project 5 - On Wednsdays, We Wear Pink

#Introduction

One thing everyone has in common (hopefully), is that we all wear clothes. Except for machines. To help give machines some of the satisfaction of wearing or knowning about clothes, six different models will be created: a Support Vector Machine Classifier, a Random Forest Classifier, an XGBoost Classifier, a
Perceptron, a Multi-Layer Neural Network, and a Convolutional Neural Network.

#Exploratory Data
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import keras
from keras.datasets import mnist
from keras.layers import Dense
from keras.models import Sequential
import tensorflow as tf
from matplotlib import pyplot as plt
from random import randint
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.svm import SVR
plt.rcParams['figure.figsize'] = [8, 8]
sns.set(color_codes=True)
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import keras
from keras.datasets import mnist
from keras.layers import Dense
from keras.models import Sequential
import tensorflow as tf
from matplotlib import pyplot as plt
from random import randint
from matplotlib import pyplot as plt
from random import randint
import numpy as np

path = '/content/drive/MyDrive/mlData/'
file = path + 'fashiontrain.csv'

fashion1 = pd.read_csv(file)
#fashion.head()
file = path + 'fashiontest.csv'
fashion2 = pd.read_csv(file)

fashion = fashion1.append(fashion2, ignore_index=True)

fashion.shape

fashion.head()

fashion['label'].describe()

fashion.isnull().any()
fashion.dropna()

fashionV = fashion
fashionR = fashion
fashionXG = fashion
fashionP = fashion

"""In my exploration, I decided to combine the train and test data sets from Kaggle into one larger data set and I will split it into train and test data myself. I did this so I could more easily manipulate the data to my prefrences and needs for my generated models. Once the two datasets were added together, they formed a data set of 70,000 rows and 785 columns. Pretty big

#Support Vector Machine
"""

fashionVx = fashionV.drop(["label"], axis = 1)
fashionVy = fashionV["label"]

fashionVx.shape

fashionVy.head()

X_train, X_test, y_train, y_test = train_test_split(fashionVx, fashionVy, test_size=0.2, random_state=10)

scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.svm import SVC

fashionSVC = SVC(kernel='rbf', C=1)
fashionSVC.fit(X_train,y_train)

y_pred = fashionSVC.predict(X_test)
print(metrics.classification_report(y_test, y_pred))

"""The support vector machine performed fairly well. The average precision for the 10 different clothes and accessories was 89%. For the simplicity of the model, a precision of 89% is pretty good.

#Random Forest Classification Model
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

fashionRx = fashionV.drop(["label"], axis = 1)
fashionRy = fashionV["label"]

X_train,X_test,y_train,y_test=train_test_split(fashionRx,fashionRy,test_size=0.2,random_state=10, stratify = fashionRy)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

fashionForest = RandomForestClassifier(n_estimators=101, max_features=500,max_depth=400, random_state=10)
fashionForest.fit(X_train, y_train)
y_pred = fashionForest.predict(X_test)

print("Accuracy on training set: {:.3f}".format(fashionForest.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(fashionForest.score(X_test, y_test)))

"""The random forest classification performed with an accuracy of 89%, pretty accurate. This model took 31 minutes to run, which was annoying because it is getting late and I want to go to sleep. This model performed worse than my Support Vector Machine.

#XGBoost Classifier
"""

import random
import xgboost as xgb

fashionXGx = fashionXG.drop(["label"], axis = 1)
fashionXGy = fashionXG["label"]

X_train,X_test,y_train,y_test=train_test_split(fashionXGx,fashionXGy,test_size=0.2,random_state=10, stratify = fashionXGy)

xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.001,
                max_depth = 9, alpha = 10, n_estimators = 100)

xg_reg.fit(X_train, y_train)

y_pred = xg_reg.predict(X_test)
print('Mean Absolute Error = ', metrics.mean_absolute_error(y_test, y_pred))
print('Mean Square Error = ', metrics.mean_squared_error(y_test, y_pred))
print('Root Mean Square Error = ', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))

print("Accuracy on training set: {:.3f}".format(xg_reg.score(X_train, y_train)))
print("Accuracy on test set: {:.3f}".format(xg_reg.score(X_test, y_test)))

xg_reg.score(X_train, y_train)

"""This thing sucks. With a root mean square error of 4.5 and an an accuracy of -144.2% somehow, this model does awful. Much, much worse than my other models.

#Perceptron
"""

from sklearn.linear_model import Perceptron

fashionPx = fashionP.drop(["label"], axis = 1)
fashionPy = fashionP["label"]
X_train, X_test, y_train, y_test = train_test_split(fashionPx, fashionPy, test_size=0.2, random_state=11)

scale = StandardScaler()
scale.fit(X_train)

X_train = scale.transform(X_train)
X_test = scale.transform(X_test)

ppn = Perceptron(max_iter=1785, eta0=0.01, random_state=10)
ppn.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

y_pred = ppn.predict(X_test)
print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix

class_names=[0,1,2] # name  of classes
fig, ax = plt.subplots(figsize=(5, 5))
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="BuPu" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion Matrix', y=1.1)
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')

"""The perceptron performed alright. With an accuracy of 79%. The perceptron model gives me hope for the neural network models. The main flaw of this function was its inability to properly identify shirts. The model mixed up shirts with T-shirts, pullovers, and coats.

#Multi-Layer Neural Network
"""

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()

print("x_train shape:", x_train.shape, "y_train shape:", y_train.shape)

print(x_train.shape)
print(x_test.shape)

image_size = 784
x_train = x_train.reshape(x_train.shape[0], image_size)
x_test = x_test.reshape(x_test.shape[0], image_size)

y_train.shape

num_classes = 10
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

model = Sequential()

model.add(Dense(units=512, activation='sigmoid', input_shape=(image_size,)))
model.add(Dense(units=512, activation='sigmoid'))
model.add(Dense(units=512, activation='sigmoid'))
model.add(Dense(units=num_classes, activation='softmax'))

model.summary()

model.compile(optimizer="sgd", loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train, batch_size=16, epochs=50, verbose=True, validation_split=.1)
loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['training', 'validation'], loc='best')
plt.show()

print(f'Test loss: {loss:.3}')
print(f'Test accuracy: {accuracy:.3}')

"""The multi-layer neural network was a bit of a letdown in this case. With a test accuracy of just 71%, it underperformed the perceptron! I think with more tweaking, this model could defenitly perform at a higher accuracy. I was worried about overfitting, but it appears that the model did not overfit because the training and validation graph shows the training and validation scores very similarly.

#Convolutional Neural Network
"""

from keras.layers import Dropout
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Activation, Conv2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, Flatten
from tensorflow.keras.layers import BatchNormalization

(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()

#print("x_train shape:", x_train.shape, "y_train shape:", y_train.shape)

X_train = X_train.reshape(60000, 28, 28, 1)
X_test = X_test.reshape(10000, 28, 28, 1)

X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

X_train /= 255
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)

num_classes = 10
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

model = Sequential()

model.add(Conv2D(32, (3, 3), input_shape=(28,28,1)))
model.add(BatchNormalization(axis=-1))
convLayer01 = Activation('relu')
model.add(convLayer01)

model.add(Conv2D(32, (3, 3)))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
convLayer02 = MaxPooling2D(pool_size=(2,2))
model.add(convLayer02)

model.add(Conv2D(64,(3, 3)))
model.add(BatchNormalization(axis=-1))
convLayer03 = Activation('relu')
model.add(convLayer03)

model.add(Conv2D(64, (3, 3)))
model.add(BatchNormalization(axis=-1))
model.add(Activation('relu'))
convLayer04 = MaxPooling2D(pool_size=(2,2))
model.add(convLayer04)
model.add(Flatten())

model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation('relu'))

model.add(Dropout(0.2))
model.add(Dense(10))
model.add(Activation('softmax'))

model.summary()

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

gen = ImageDataGenerator(rotation_range=8, width_shift_range=0.08, shear_range=0.3,
                         height_shift_range=0.08, zoom_range=0.08)

test_gen = ImageDataGenerator()

train_generator = gen.flow(X_train, y_train, batch_size=128)
test_generator = test_gen.flow(X_test, y_test, batch_size=128)

model.fit(train_generator, steps_per_epoch=60000//128, epochs=15, verbose=1,
                    validation_data=test_generator, validation_steps=10000//128)

score = model.evaluate(X_test, y_test)
print('Test score:', score[0])
print('Test accuracy:', score[1])

"""The convolutional neural network performed pretty good with an accuracy of 91% after running for 47 minutes. After 14 iterations, the accuracy went down on the last epoch which is interesting. With more tweaking, I believe this model would have performed much better, but I do not have the patience.

#Conclusion

Out of the six created models, the best performing one was the convolutional neural network, which makes sense because neural networks are very good at computer vision. The worst performing model was the random forest. I think the decison tree did the worst due to the design of decision trees, making decisions after single pixels does not make the most sense for visual learning and identifying objects in pictures. This project has increased my understanding in the six different types of machine learning models generated.
"""